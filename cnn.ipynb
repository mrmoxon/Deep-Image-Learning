{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural-Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the images to [0, 1]\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dropout, Dense\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the images to add a dimension for the channel\n",
    "x_train_cnn = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test_cnn = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "# Define the CNN model\n",
    "cnn_model = Sequential([\n",
    "    Conv2D(32, kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1)), # Input shape: this is the shape of one image, it has 1 dimension (greyscale)\n",
    "    MaxPool2D(pool_size=(2, 2)),           # This layer performs max pooling with a 2 x 2 filter, reducing the dimensions (height and width) of the feature maps by half.\n",
    "\n",
    "    Conv2D(32, (3, 3), activation='relu'), # This layer applies 32 differnt filters (each 3 x 3), each filter extracts different features (edges, corners, etc.)\n",
    "                                           # ReLU activation is used to introduce non-linearity, allowing the model to learn more complex patterns. \n",
    "\n",
    "    MaxPool2D(pool_size=(2, 2)),           # This layer performs max pooling with a 2 x 2 filter, reducing the dimensions (height and width) of the feature maps by half.\n",
    "\n",
    "    Flatten(),                             # This layer flattens the 2D feature maps into a 1D vector, which can be used as input to a fully connected neural network.\n",
    "\n",
    "    Dense(256, activation='relu'),         # This layer has 256 neurons and uses ReLU activation.\n",
    "\n",
    "    Dropout(0.5),                          # Regularisation technique where randomly selected neurons are ignored during training. This helps prevent overfitting.\n",
    "\n",
    "    Dense(10, activation='softmax')        # This layer has 10 neurons and uses softmax activation. Softmax activation enables you to calculate probabilities for each class score.\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "188/188 [==============================] - 4s 20ms/step - loss: 0.4141 - accuracy: 0.8719 - val_loss: 0.0990 - val_accuracy: 0.9701\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - 4s 19ms/step - loss: 0.1146 - accuracy: 0.9665 - val_loss: 0.0627 - val_accuracy: 0.9818\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - 4s 20ms/step - loss: 0.0809 - accuracy: 0.9760 - val_loss: 0.0521 - val_accuracy: 0.9852\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - 4s 20ms/step - loss: 0.0652 - accuracy: 0.9803 - val_loss: 0.0445 - val_accuracy: 0.9868\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - 4s 20ms/step - loss: 0.0539 - accuracy: 0.9835 - val_loss: 0.0413 - val_accuracy: 0.9890\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - 4s 20ms/step - loss: 0.0458 - accuracy: 0.9856 - val_loss: 0.0374 - val_accuracy: 0.9898\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - 4s 19ms/step - loss: 0.0401 - accuracy: 0.9874 - val_loss: 0.0364 - val_accuracy: 0.9902\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - 4s 20ms/step - loss: 0.0369 - accuracy: 0.9883 - val_loss: 0.0361 - val_accuracy: 0.9901\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - 4s 20ms/step - loss: 0.0347 - accuracy: 0.9886 - val_loss: 0.0357 - val_accuracy: 0.9901\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - 4s 19ms/step - loss: 0.0311 - accuracy: 0.9900 - val_loss: 0.0350 - val_accuracy: 0.9898\n"
     ]
    }
   ],
   "source": [
    "# Train the CNN model\n",
    "cnn_history = cnn_model.fit(x_train_cnn, y_train, epochs=10, batch_size=256, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0265 - accuracy: 0.9913\n",
      "CNN Test Accuracy: 0.9912999868392944\n"
     ]
    }
   ],
   "source": [
    "# Evaluate CNN model\n",
    "cnn_loss, cnn_accuracy = cnn_model.evaluate(x_test_cnn, y_test)\n",
    "print(\"CNN Test Accuracy:\", cnn_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
